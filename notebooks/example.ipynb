{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper and lower bound estimators for NInGa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides example code for how to compute the upper and lower bound estimators for the performance measure `NInGa`. The distributions we use in this notebook are the `Zero Inflated Gamma` and the `Zero Inflated LogNormal` distribution. Other distributions can be implemented similarly. For derivations, see the paper:\n",
    "\n",
    "[Bayesian Oracle for bounding information gain in neural encoding models](https://openreview.net/forum?id=iYC5hOMqUg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from neuralmetrics.datasets import simulate_neuron_data, simulate_neuron_data_advanced\n",
    "from neuralmetrics.models.utils import get_zig_params_from_moments, get_zil_params_from_moments\n",
    "from neuralmetrics.utils import bits_per_image\n",
    "from neuralmetrics.models.gs_models import Gaussian_GS, Gamma_GS\n",
    "from neuralmetrics.models.gs_zero_inflation import Zero_Inflation_Base\n",
    "from neuralmetrics.models.priors import get_prior_for_gaussian, get_prior_for_q, get_prior_for_gamma, train_prior_for_gaussian\n",
    "from neuralmetrics.models.flows.transforms import Log, Identity\n",
    "from neuralmetrics.models.score_functions import compute_gs_loss_over_target_repeats, compute_null_loss\n",
    "from neuralpredictors.measures.zero_inflated_losses import ZIGLoss, ZILLoss\n",
    "\n",
    "from scipy.stats import beta as beta_distribution\n",
    "\n",
    "from neuralpredictors.measures import corr\n",
    "from neuralpredictors.measures.zero_inflated_losses import ZILLoss\n",
    "\n",
    "\n",
    "random_seed = 27121992\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simulate neural data to show the functionality of the code on. Plug in your own code for loading your data instead. In the end, the data needs to be a numpy array of the shape `(n_repeats, n_images, n_neurons)`. If trials are missing in your data, replace them by `np.nan`. Note that for a missing trial, all entries in the neuron dimension need to be missing (see also the end of the following cell for an example). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "\n",
    "exp_data = True\n",
    "n_images = 360\n",
    "n_repeats = 10\n",
    "n_neurons = 100\n",
    "\n",
    "mean = .5\n",
    "variance = .01\n",
    "A = (mean * (1 - mean) / variance - 1)\n",
    "alpha = A * mean\n",
    "beta = A * (1 - mean)\n",
    "zero_inflation_level = beta_distribution(21, 117).rvs(n_neurons)\n",
    "loc = np.exp(-10)\n",
    "\n",
    "resps, gt_means, gt_variances, zil_params = simulate_neuron_data_advanced(n_images=n_images,\n",
    "                                                      n_repeats=n_repeats,\n",
    "                                                      n_neurons=n_neurons,\n",
    "                                                      zero_inflation_level=zero_inflation_level,\n",
    "                                                      loc=loc,\n",
    "                                                      random_state=random_seed)\n",
    "\n",
    "# If single trials are missing due to experimental errors, replace them by np.nan, for example:\n",
    "resps[0, 0, :] = np.nan\n",
    "n_trials = (n_repeats*n_images*n_neurons - np.isnan(resps).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize prior params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper bound estimator (GS model) is best, if the prior hyperparamters are optimized. This takes a long time but only needs to be done once per dataset. The following cells show how to do this for the examples of the `Zero Inflated Gamma` and the `Zero Inflated LogNormal` distribution. Note that we choose reasonable initialization values for the prior hyperparameters by fitting to the raw data using the functions `get_prior_for_q`, `get_prior_for_gaussian` and `get_prior_for_gamma`. This is not necessary but speeds up the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distribution = \"Zero Inflated Gamma\" #\"Zero Inflated LogNormal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize GS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loc = np.exp(-10)\n",
    "slab_mask = np.ones_like(resps)\n",
    "slab_mask[resps <= loc] = np.nan\n",
    "print(\"Getting good init values for q prior parameters...\")\n",
    "q_prior_params = get_prior_for_q(torch.from_numpy(resps), loc)\n",
    "\n",
    "# Initialize GS model\n",
    "if distribution == \"Zero Inflated LogNormal\":\n",
    "    transform = Log()\n",
    "    resps_transformed, _ = transform(torch.from_numpy(resps) - loc)\n",
    "    print(\"Getting good init values for slab prior parameters...\")\n",
    "    slab_prior_params = get_prior_for_gaussian(resps_transformed.numpy(),\n",
    "                                                   per_neuron=False,\n",
    "                                                   mask=slab_mask,\n",
    "                                                   lr_decay_steps=1)\n",
    "    dist_slab = Gaussian_GS(*slab_prior_params, train_prior_hyperparams=True, alpha_greater_one=True)\n",
    "    \n",
    "elif distribution == \"Zero Inflated Gamma\":\n",
    "    transform = Identity()\n",
    "    resps_transformed, _ = transform(torch.from_numpy(resps) - loc)\n",
    "    print(\"Getting good init values for slab prior parameters...\")\n",
    "    slab_prior_params = get_prior_for_gamma(resps_transformed.numpy(),\n",
    "                                                   per_neuron=False,\n",
    "                                                   mask=slab_mask)\n",
    "    dist_slab = Gamma_GS(*slab_prior_params, train_prior_hyperparams=True)\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "possible_number_of_loo_repeats = np.unique([dist_slab.get_number_of_repeats(torch.from_numpy(resps[:, i, :])) - 1 for i in range(resps.shape[1])])\n",
    "gs_model = Zero_Inflation_Base(\n",
    "    loc,\n",
    "    dist_slab,\n",
    "    *q_prior_params,\n",
    "    possible_number_of_loo_repeats=possible_number_of_loo_repeats,\n",
    "    transform=transform,\n",
    ").to(device)\n",
    "gs_model.integrals_over_q_dict = gs_model.get_integrals_over_q()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize prior params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell takes a long time. The warning about not forgetting to recompute the integral over q can be ignored because it is taken care of. Consider saving the optimized prior hyperparameters after the optimization is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Optimizing prior parameters...\")\n",
    "gs_model, loss = train_prior_for_gaussian(resps, gs_model, max_iter=200, logger=False, use_map=False)\n",
    "\n",
    "# Optionally save optimized prior params\n",
    "prior_params = {k: v for k, v in gs_model.named_parameters()}\n",
    "# torch.save(prior_params, \"optimized_prior_params\" + \".tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain upper and lower bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if distribution == \"Zero Inflated LogNormal\":\n",
    "    params_from_moments_function = get_zil_params_from_moments\n",
    "    loss_function = ZILLoss(per_neuron=True)\n",
    "\n",
    "elif distribution == \"Zero Inflated Gamma\":\n",
    "    params_from_moments_function = get_zig_params_from_moments\n",
    "    loss_function = ZIGLoss(per_neuron=True)\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "# Get upper bound log-likelihood per repeat, image and neuron\n",
    "loss_gs = compute_gs_loss_over_target_repeats(resps, gs_model, False).item()\n",
    "upper_bound = -loss_gs / n_trials\n",
    "\n",
    "# Get lower bound log-likelihood per repeat, image and neuron\n",
    "loss_null = compute_null_loss(resps, params_from_moments_function, loss_function, torch.Tensor([loc]).to(device), device).sum()\n",
    "lower_bound = -loss_null / n_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"upper_bound: {upper_bound}\")\n",
    "print(f\"lower_bound: {lower_bound}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
